# Machine Learning Module by Fayyaz Minhas

## Video Playlist: [<span class="underline">https://www.youtube.com/playlist?list=PL9IcorxiyRbASB9DXjoWnBJO9RSKyzM2N</span>](https://www.youtube.com/playlist?list=PL9IcorxiyRbASB9DXjoWnBJO9RSKyzM2N) 

## Books

\[PML\] Probabilistic Machine Learning: An Introduction by Kevin Patrick
Murphy. MIT Press, 2021. link:
[<span class="underline">http://mlbayes.ai/</span>](http://mlbayes.ai/)

\[IML\] Introduction to Machine Learning 3e by Ethem Alpaydin (selected
chapters: ch. 1,2,6,7,9,10,11,12,13)

\[DBB\] Deep Learning by Ian Goodfellow, Yoshua Bengio, Aaron Courville,
(Ch 1-5 if needed as basics), Ch. 6,7,8,9 link:
[<span class="underline">https://www.deeplearningbook.org/</span>](https://www.deeplearningbook.org/)

\[FNN\] Fundamentals of Neural Networks : Architectures, Algorithms And
Applications by Laurene Fausett, (ch. 2,6)

## Module Contents

<table>
<thead>
<tr class="header">
<th>1</th>
<th><p>1.1 Introduction</p>
<p>1.2 Why Data Science?</p>
<p>1.3 Applications</p>
<p>1.4 Research Applications</p>
<p>1.5 Framework</p></th>
<th><p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-1-introduction.pdf"><span class="underline">Introduction Slides</span></a></p>
<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-2-machine_learning_framework.pdf"><span class="underline">Applications and Framework Slides</span></a></p>
<p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"><span class="underline">k-Nearest Neighbor Algorithm</span></a> [Required]</p>
<p>[PML] Chapter-1, [IML] Chapter-1</p>
<p><a href="https://youtu.be/4TB4Z8_T3d8"><span class="underline">CRISPR Talk</span></a></p>
<p><a href="https://youtu.be/Of1u0i7roS0"><span class="underline">Whole Slide Images are Graphs Talk</span></a></p>
<p>The master algorithm (casual reading)</p>
<p><a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"><span class="underline">A few useful things to know about machine learning</span></a></p></th>
<th><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/#learningPython"><span class="underline">Learning Python</span></a></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td><p>2.1 Classification and Linear Discriminants</p>
<p>2.2 Determining Linear Separability</p>
<p>2.3 Prelim: Gradients and Gradient descent</p>
<p>2.4 Prelim: Gradient Descent Code</p>
<p>2.5 Prelim: Convexity</p>
<p>2.6 Perceptron Modeling</p>
<p><a href="https://web.microsoftstream.com/video/d84e1a2d-3e5e-4d05-b35f-ffac56c83025"><span class="underline">Perceptron Code</span></a></p></td>
<td><p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-3_linear_discriminants.pdf"><span class="underline">Linear Discriminants (notes)</span></a></p>
<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/gradients_gradient_descent_and_convexity.pdf"><span class="underline">Preliminaries (notes)</span></a></p>
<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-4_building_linear_models.pdf"><span class="underline">Building Linear Models (notes)</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/gd.py"><span class="underline">Gradient Descent Code (py)</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/perceptron.py"><span class="underline">Perceptron Code (py)</span></a></p>
<p><a href="https://en.wikipedia.org/wiki/Perceptron"><span class="underline">Perceptron Algorithm</span></a></p></td>
<td><a href="https://github.com/foxtrotmike/CS909/blob/master/DM_1_kNN.ipynb"><span class="underline">Implementing kNN classifier</span></a></td>
</tr>
<tr class="even">
<td>W</td>
<td>Lectures</td>
<td>Reading/Resources</td>
<td>Labs</td>
</tr>
<tr class="odd">
<td>3</td>
<td><p>3.1 What's special in an SVM?</p>
<p>3.2 SVM Formulation</p>
<p>3.3 A brief history of SVMs</p>
<p>3.4 Coding an SVM and C</p>
<p>3.5 Margin and Regularization</p>
<p>3.6 Linear Discriminants and Selenite Crystals</p>
<p>3.7 Selenit Crystals bend Space</p>
<p>3.8 Using transformations to fold</p>
<p>3.9 Transformations change distance and dot products</p>
<p>3.10 Kernelized SVMs</p></td>
<td><p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-5_perceptron_to_svm.pdf"><span class="underline">SVM Notes</span></a></p>
<p><a href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/"><span class="underline">SVM Applet</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/regper.ipynb"><span class="underline">Regularized Perceptron</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/transformations.ipynb"><span class="underline">Transformations code</span></a></p>
<p><a href="https://youtu.be/ZREp1mAPKTM"><span class="underline">Fold and Cut Theoreom</span></a></p>
<p>Book Reading [SVM in PML, SVM in IML]</p>
<p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000173"><span class="underline">SVM Tutorial</span></a></p></td>
<td><p><a href="https://github.com/foxtrotmike/CS909/blob/master/dm_lab_2_fm.ipynb"><span class="underline">Gradient Descent and Perceptron</span></a></p>
<p>(see lectures from previous week)</p>
<p><a href="https://github.com/foxtrotmike/svmtutorial/blob/master/svmtutorial.ipynb"><span class="underline">SVM</span></a></p>
<p>Assignment-1 Announced</p></td>
</tr>
<tr class="even">
<td>4</td>
<td><p>4.1 Scientific Method</p>
<p>4.2 Why measure performance?</p>
<p>4.3 Accuracy and its assumptions</p>
<p>4.4 Confusion matrix and associated metrics</p>
<p>4.5 ROC Curves</p>
<p>4.6 PR Curves</p>
<p>4.7 PR-ROC Relationship and coding</p>
<p>4.8 Estimating Generalization</p>
<p>4.9 CV in sklearn</p></td>
<td><p>Chapter 19 “Design and Analysis of Machine Learning Experiments” Alpaydin, Ethem. 2010. Introduction to Machine Learning. Cambridge, Mass.: MIT Press.</p>
<p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-6_evaluating_and_comparing_models.pdf"><span class="underline">Lecture Notes</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/evaluation_example.ipynb"><span class="underline">Performance Assessment Exercise</span></a></p>
<p>Optional Reading</p>
<p>Wainberg, Michael, Babak Alipanahi, and Brendan J. Frey. “Are Random Forests Truly the Best Classifiers?” Journal of Machine Learning Research 17, no. 110 (2016): 1–5.</p>
<p>Munir, Farzeen, Sadaf Gull, Amina Asif, and Fayyaz Ul Amir Afsar Minhas. “MILAMP: Multiple Instance Prediction of Amyloid Proteins.” IEEE/ACM Transactions on Computational Biology and Bioinformatics, August 22, 2019. https://doi.org/10.1109/TCBB.2019.2936846.</p></td>
<td><p><a href="https://github.com/foxtrotmike/svmtutorial/blob/master/svmtutorial.ipynb"><span class="underline">SVM</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/evaluation_example.ipynb"><span class="underline">Performance Assessment Exercise</span></a></p>
<p>Work on Assignment-1</p></td>
</tr>
<tr class="odd">
<td>5</td>
<td><p>5.1 Twelve ways to fool the masses</p>
<p>5.2.1 Prelim: Old MacDonald meets Lagrange</p>
<p>5.2. Prelim: Meet stubborn vectors</p>
<p>5.2.3 Prelim: Covariance and its friends</p>
<p>5.3 PCA</p></td>
<td><p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-da-5-pca.pdf"><span class="underline">Lecture Notes</span></a></p>
<p><a href="https://arxiv.org/abs/1901.01686"><span class="underline">Ten ways to fool the masses with machine learning</span></a></p>
<p><a href="https://github.com/foxtrotmike/PCA-Tutorial/blob/master/Eigen.ipynb"><span class="underline">Eigen Values and Vectors</span></a></p>
<p><a href="https://github.com/foxtrotmike/PCA-Tutorial/blob/master/pca-lagrange.ipynb"><span class="underline">PCA Tutorial</span></a></p></td>
<td><p><a href="https://github.com/foxtrotmike/PCA-Tutorial/blob/master/Eigen.ipynb"><span class="underline">Eigen Values and Vectors</span></a></p>
<p><a href="https://github.com/foxtrotmike/PCA-Tutorial/blob/master/pca-lagrange.ipynb"><span class="underline">PCA Tutorial</span></a></p>
<p>Work on Assignment-1</p></td>
</tr>
<tr class="even">
<td>6</td>
<td><p>6.1 Other dimensionality reduction methods</p>
<p>6.2 SRM view of PCA</p>
<p>6.3 OLSR</p>
<p>6.4 OLSR to SVR</p>
<p>6.5 Hurricane Intensity Regression</p></td>
<td><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-9_regression.pdf"><span class="underline">Lecture Notes</span></a></td>
<td><p>Work on Assignment-1</p>
<p>(Assignment-2 posted <a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/cs909_2021_assignment_2.pdf"><span class="underline">Download Assignment Description</span></a>)</p></td>
</tr>
<tr class="odd">
<td>7</td>
<td><p>7.1 How to go beyond classification, regression and dimensionality reduction</p>
<p>7.2 Applied SRM in Barebones Pytorch</p>
<p>7.3 Clustering</p>
<p>7.4 Clustering in sklearn</p>
<p>7.5 One class classifiers</p>
<p>7.6 Ranking</p>
<p>7.7 Recommender Systems</p>
<p>7.8 Still more problems</p></td>
<td><p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/minhas-ml-10_other_machines.pdf"><span class="underline">Lecture Notes</span></a></p>
<p><a href="https://www.youtube.com/watch?v=4TB4Z8_T3d8"><span class="underline">Finding Anti-CRISPR proteins with ranking (optional)</span></a></p>
<p><a href="https://youtu.be/N20h6vpR13Y"><span class="underline">Using reinforcement learning to help a mouse escape a cat (optional)</span></a></p></td>
<td><p><a href="https://github.com/foxtrotmike/CS909/blob/master/barebones.ipynb"><span class="underline">Barebones Linear Models</span></a></p>
<p><a href="https://github.com/shaneahmed/StatswithPython/blob/main/clustering.ipynb"><span class="underline">Clustering</span></a></p>
<p>Work on Assignment-2 (<a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/cs909_2021_assignment_2.pdf"><span class="underline">Download Assignment Description</span></a>)</p></td>
</tr>
<tr class="even">
<td>8</td>
<td><p>8.1 Let me pick your brain</p>
<p>8.2 Single Neuron Model</p>
<p>Revisit 7.2 Applied SRM in Barebones Pytorch</p>
<p>8.3 Multilayer Perceptron</p>
<p>8.4 Let's play with a neural network</p>
<p>8.5 Deriving Backpropagation algorithm for MLPs</p>
<p>8.6.1 MLP in Keras</p>
<p>8.6.2 MLP in PyTorch using NN module</p>
<p>8.6.3 MLP in PyTorch for MNIST with Dataloaders</p>
<p>8.7 Improving learning of MLPs</p></td>
<td><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/afsar-ml-13-week-8_deep_learning.pdf"><span class="underline">Lecture Notes</span></a></td>
<td><p>Work on Assignment-2 (<a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/cs909_2021_assignment_2.pdf"><span class="underline">Download Assignment Description</span></a>)</p>
<p>--</p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/barebones.ipynb"><span class="underline">Barebones Linear Models</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/keras_barebones.ipynb"><span class="underline">Keras Barebones</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/pytorch_nn_barebones.ipynb"><span class="underline">NN module in Pytorch</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/pytorch_mlp_mnist.ipynb"><span class="underline">MNIST MLP in PyTorch</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/trees.ipynb"><span class="underline">Trees and XGBoost</span></a></p>
<p>Solve the XOR using a single hidden layer BPNN with sigmoid activations</p></td>
</tr>
<tr class="odd">
<td>9</td>
<td><p>9.1 We can approximate the universe</p>
<p>9.2 By going deep</p>
<p>9.3 Finding Waldo is difficult with a fully connecteed MLP</p>
<p>9.4 Convolution</p>
<p>9.5 Learning filters</p>
<p>9.6 CNNs</p>
<p>9.7 CNN training in PyTorch</p>
<p>9.8 Why CNNs</p>
<p>9.9 CNN Hyperparameters and Regularization</p>
<p>9.10 Transfer Learning and Application</p></td>
<td><p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/afsar-ml-13_week-9_deep_learning.pdf"><span class="underline">Lecture Notes</span></a></p>
<p><a href="http://wrap.warwick.ac.uk/129159/"><span class="underline">Deep PHURIE for hurricane intensity prediction</span></a></p></td>
<td><p>Work on Assignment-2 (<a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/cs909_2021_assignment_2.pdf"><span class="underline">Download Assignment Description</span></a>)</p>
<p>-- Notes --</p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/uniapprox.ipynb"><span class="underline">Universal Approximation Code</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/pytorch_conv.py"><span class="underline">Convolution in PyTorch</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/learn_filters.py"><span class="underline">Learning a single convolution Filter</span></a></p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/0_to_AI_in_10_Lines_of_Python.ipynb"><span class="underline">0 to AI in 10 lines of code</span></a> (By J. Pocock!)</p>
<p><a href="https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/"><span class="underline">Digit Classification with CNNs in Keras</span></a></p>
<p><a href="https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py"><span class="underline">Digit Classification with CNNs in PyTorch</span></a></p>
<p><a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"><span class="underline">Transfer Learning in PyTorch</span></a></p></td>
</tr>
<tr class="even">
<td>10</td>
<td><p>10.1 Under the hood view of deep learning libraries</p>
<p>10.2 Residual Networks (and other types)</p>
<p>10.3 Autoencoders</p>
<p>10.4 Generative Models</p>
<p>10.5 GANs</p>
<p>10.6 Barebones GAN in PyTorch</p>
<p>10.7 Natural Language Modelling</p>
<p>10.8 Using GANs for generating histology images (By S. Deshpande)</p>
<p>10.9 Using Graph Neural Netwoks for histology images</p></td>
<td><p><a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/afsar-ml-13_deep_learning_complete.pdf"><span class="underline">Lecture Notes</span></a> (All deep learning Notes)</p>
<p><a href="https://github.com/foxtrotmike/CS909/blob/master/simpleGAN.ipynb"><span class="underline">GAN in PyTorch</span></a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/"><span class="underline">NLP</span></a></p></td>
<td>Work on Assignment-2 (<a href="https://warwick.ac.uk/fac/sci/dcs/teaching/material/cs909/cs909_2021_assignment_2.pdf"><span class="underline">Download Assignment Description</span></a>)</td>
</tr>
</tbody>
</table>

##   

## Self Assessment Exercise Questions

### Week-2

Go through the lectures for Week-2 (uploaded on the module webpage) and
then answer the following questions.

1\. What is meant by a classifier? What is meant by the term
"generalization"?

2\. What is meant by a discriminant?

3\. What is a linear discriminant? What is the mathematical
representation of a linar discriminant?

4\. What is meant by linear separability of a dataset?

5\. Determine if each of the following datasets is linearly separable or
not. If so, determine the linear discriminant for each dataset and
calculate the score of the linear discriminant for each example in each
dataset to verify your answer.

AND

(0,0) label: -1

(0,1) label: -1

(1,0) label: -1

(1,1) label: +1

OR

(0,0) label: -1

(0,1) label: +1

(1,0) label: +1

(1,1) label: +1

XOR

(0,0) label: -1

(0,1) label: +1

(1,0) label: +1

(1,1) label: -1

6\. Determine if the following dataset is linearly separable or not. If
so, determine the linear discriminant for this dataset and calculate the
score of the linear discriminant for each of the following examples to
verify your answer.

(0,0,0), label = +1

(0,0,1), label = -1

(0,1,0), label = -1

(0,1,1), label = -1

(1,0,0), label = -1

(1,0,1), label = -1

(1,1,0), label = -1

(1,1,1), label = -1

7\. Determine if the following dataset is linearly separable or not. If
so, determine the linear discriminant for this dataset and calculate the
score of the linear discriminant for each of the following examples to
verify your answer.

(0,0,0), label = +1

(0,0,1), label = -1

(0,1,0), label = -1

(0,1,1), label = -1

(1,0,0), label = -1

(1,0,1), label = -1

(1,1,0), label = -1

(1,1,1), label = +1

8\. Determine if the following dataset is linearly separable or not. If
so, determine the linear discriminant for this dataset and calculate the
score of the linear discriminant for each of the following examples to
verify your answer.

(0,0,0), label = +1

(0,0,1), label = -1

(0,1,0), label = -1

(0,1,1), label = +1

(1,0,0), label = -1

(1,0,1), label = +1

(1,1,0), label = +1

(1,1,1), label = -1

9\. Do you think there can be multiple linear discriminant solutions to
a classification problem?

10\. What is the role of the bias term in the discriminant function?

11\. What would be limitations imposed by the lack of a bias term in the
discriminant function, i.e., what would happen if we use f(x)=w^Tx
instead of f(x)=w^Tx+b?

12\. Calculate the analytical form of the gradient vector of the
following function (with respect to variables a and b):

f(a,b)=(a-b)^2

f(a,b)=a^2-ab

f(a,b)=a-ab

13\. What is meant by convexity? Use your understanding of function
convexity to determine if each of the following functions is convex or
not.

f(x)=2x+3

f(x)=(x-2)^2+4

f(x)=4(x-2)^3+3

f(x)=sin(x)

14\. Describe, in your own word, the gradient descent algorithm?

15\. What is the role of the learning rate or step size parameter in the
gradient descent algorithm? Use the provided code to understand the role
of this parameter.

16\. Use the provided gradient descent code to calculate the minima of
each of the following functions. Verify if the answer is correct by
solving using calculus (calculating the derivative and setting it to
zero by hand).

f(x) = 2x^2-x-2

f(x) = sin(3x)

f(x)=2x+3

f(x)=(x-2)^2+4

f(x)=4(x-2)^3+3

f(x)=sin(x)

17\. What are the limitations of the gradient descent algorithm?

18\. Why does the gradient descent algorithm fail for non-convex
functions? How can we remedy this issue?

19\. What is the 0-1 loss? What are the problems associated with the 0-1
loss function?

20\. What is the hinge loss function?

21\. What is the perceptron algorithm? What are the representation,
evaluation and optimization components of the perceptron?

22\. Use the provided perceptron code to understand the role of the
learning rate parameter by solving simple AND, OR and XOR classification
problems.

23\. What is the effect of the choice of initial weights and biases in
the perceptron code? Modify the code and try w

24\. Use the perceptron algorithm to solve the classification exercises
given in Q.5-Q.8.

25\. Use the builtin perceptron algorithm in sk-learn
(<https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html>)
to solve the classification exercises given in Q.5-Q.8.

26\. What is the perceptron learning rule convergence theorem?

27\. Do you think that the perceptron algorithm can be a universal
computing machine?
(<https://en.wikipedia.org/wiki/Universal_Turing_machine>) Give your
reasoning.

###   

### Week-3

1.  > How is margin related to regularization?

2.  > Why is regularization needed?

3.  > How is structural risk minimization different from empirical risk
    > minimization?

4.  > What is role of the parameter "C" in an SVM?

5.  > As discussed in the lectures, some SVM formulations use "Lambda"
    > in their expression instead of "C". What is the relationship
    > between "C" and "Lambda"?

6.  > What is a support vector?

7.  > Write the weight update step from gradient descent for an SVM?

8.  > How can you determine if the objective function in the
    > optimization problem underlying an SVM is convex or non-convex?

9.  > What are the advantages of linear discriminant based classifier
    > such as perceptron or a support vector machine?

10. > How do transformations of data points change the concept of
    > distance in the feature space?

11. > How do transformations of data points allow us to use a linear
    > discriminant in the transformed space to solve an originally
    > linearly non-separable classification problem?

12. > What is the relationship between distance and dot products?

13. > How can we achieve implicit transformations of the data by
    > changing the definition of dot products to kernel functions?

14. > What is a kernel function?

15. > What is the representer theorem?

16. > How does the representer theoreom allow us to use kernels in an
    > SVM?

17. > Write the discriminant function of a support vector machine using
    > the representer theoreom?

18. > Obtain an expression for the optimization problem underlying a
    > kernelized support vector machine in terms of the data points and
    > the representer theorem.

19. > Solve the optimization problem underlying a kernelized support
    > vector machine with respect to "alpha" using gradient descent
    > algorithm by writing the update expression for alpha.

20. > What is the Gram matrix?

21. > What are the conditions for a function to be a valid kernel
    > function?

22. > What is the role of the degree (d) and coefficient (c) in a
    > polynomial kernel k(a,b)=(a^t b + c)^d? What impact do these have
    > on the classification boundary?

23. > What is the role of the parameter "gamma" in an RBF kernel? How
    > does it affect the classification boundary?

24. > How can you specify your own kernel function in the SVM?

25. > How does using a kernel matrix eliminate the need for explicit
    > feature representation of examples for a classification problem?

### Week-4

1.  > What is the objective of validation?

2.  > What is stratified validation?

3.  > What are underlying assumptions for accuracy as a metric?

4.  > What is precision, recall, false positive rate?

5.  > Why are accuracy, precision, recall etc. dependent upon the
    > threshold of the classifier?

6.  > How do precision, recall and false positive rate change as the
    > threshold of the classifier is increased?

7.  > What is the ROC curve?

8.  > How does area under the ROC curve serve as a performance metric?

9.  > Why is the ROC curve called the ROC curve?

10. > How does the performance estimate of your model change with
    > increase in the size of your validation set?

11. > What are the limitations of ROC curves?

12. > What is the most important region of an ROC curve?

13. > How is a precision recall curve useful?

14. > What is the relationship between the ROC and Precision-Recall
    > curves?

15. > What are the limitations of the precision recall curve?

16. > How do we train the final model for deployment?

17. > How can you choose an "operating point" for a machine learning
    > model?

18. > What is the impact of the choice of K in K-fold cross validation
    > on performance statistics?

19. > What K should we use?

20. > What is F1?

21. > What is Matthews Correlation Coefficient?

22. > Why are FPR and TPR monotonically non-increasing functions of
    > threshold but precision is not?

23. > What is grid search?

24. > What is bootstrapping? What are .632 and .632+ bootstrap?

Week-5

A number of questions are included in the PCA Lecture files and the
eigen decomposition and PCA tutorials linked below:

[<span class="underline">https://github.com/foxtrotmike/PCA-Tutorial/blob/master/Eigen.ipynb</span>](https://github.com/foxtrotmike/PCA-Tutorial/blob/master/Eigen.ipynb)

[<span class="underline">https://github.com/foxtrotmike/PCA-Tutorial/blob/master/pca-lagrange.ipynb</span>](https://github.com/foxtrotmike/PCA-Tutorial/blob/master/pca-lagrange.ipynb)

Week-6

What is meant by a manifold?

What is the SRM formulation of PCA?

What is the primary idea behind incremental PCA?

1.  > What is the fundamcental idea of robust regression?

2.  > What is UMAP? How does it work? What is the role of the
    > neighborhood and distance constraint parameter for UMAP?

3.  > What is meant by t-SNE? How does it work?

4.  > How can PCA be kernelized?

5.  > What does "Component Analysis" mean in general? How is it
    > different from "Discriminant Analysis"?

6.  > What is meant by regression? How is it different from
    > classification?

7.  > Derive a closed-form formula for the optimal weights of ordinary
    > least squares regression?

8.  > Can OLS be used for classification? Give a justification of why
    > this is or is not a good choice.

9.  > What is the difference in terms of Representation, Evaluation and
    > Optimization between each of the following models: Ordinary Least
    > Squares Regression, Ridge Regression, Lasso Regression, Support
    > Vector Regression, Logistic Regression?

10. > Is Logistic Regression a regression method or a classification
    > method? Given an explanation for this in terms of the loss
    > function used in logistic regression?

11. > What are the limitations of square error loss?

12. > What is the motivation behind epsilon-insensitive loss function?

13. > What is meant by pseudo-inverse of a matrix?

14. > How can you improve hurricane intensity prediction?

15. > How is ordinary least squares regression related to solving a
    > system of linear equations?

Week-7

1.  > Write the represenation and evaluation of different types of loss
    > functions used in various problems discussed in this week.

2.  > What are the desired characteristics of loss function for a given
    > problem?

3.  > What is the difference between L0, L1 and L2 regularization?

4.  > What is the impact of L0, L1 and L2 regularization?

5.  > What is the main idea behind: ranking, reinforcement learning,
    > survival prediction, etc.

6.  > What are the performance metrics for each type of machine learning
    > problem discussed in this week?

7.  > How can you use click data to obtain training data in the design
    > of recommendation systems?

8.  > Why is collaborative filtering called "collaborative" filtering?

9.  > How can you use cross-validation and other performance assessment
    > techniques for different types of ML problems discussed in this
    > week?

10. > Can you describe the process of obtaining solution to a novel
    > machine learning problem?

Week-8

1.  > What is meant by a neuron, soma, axon, dendrites, synaptic gap?

2.  > What is meant by "firing" of a neuron?

3.  > What is the mathematical model of a neuron?

4.  > What is meant by activation function?

5.  > What is the role of the activation function?

6.  > What is a neural network?

7.  > What is meant by a fully connected feed-forward neural network?

8.  > Write the representation of a fully connected neural network in
    > mathematical form for an input x?

9.  > What is the evaluation of a FCNN?

10. > How can we optimize a neural network?

11. > What is meant by a layer of neurons?

12. > What is the impact of adding neurons to a neural layer?

13. > What is the impact of adding more layers?

14. > Would a neural network with multiple layers and multiple neurons
    > in each layer be able to classify a dataset that is not linearly
    > separable?

Week-9

1.  > What is universal approximation?

2.  > How are deeper models more compact representations?

3.  > What is the effect of adding layers in a neural network?

4.  > What is the impact of adding more neurons in a layer in a neural
    > network?

5.  > What are the problems with using a fully connected neural network
    > or a Multi-layered perceptron for image detection and
    > classification?

6.  > What is convolution?

7.  > What is a filter?

8.  > How is the difference between convolution and correlation?

9.  > How can convolution be viewed as a dot product?

10. > What is a CNN? What are its major components?

11. > What is Pooling?

12. > What is stride?

13. > what is padding?

14. > What is meant by a receptive field?

15. > How can you calculate the number of features at the output of
    > convolutional blocks for flattening in a CNN?

16. > What is weight decay?

17. > How is regularization achieved in CNNs?

18. > What is drop out?

19. > What is batch normalization?

20. > What is transfer learning?

Week-10

1.  > What is a residual network layer?

2.  > Why is a resnet better at modeling?

3.  > What is generative modeling?

4.  > What is the REO for autoencoders?

5.  > What is the fundamental idea behind GANs?

6.  > What is stop-word remove in NLP applications?

7.  > What is stemming?

8.  > What is leammtization?

9.  > What is meant by a dictionary for a corpus?

10. > What is meant by term-frequency?

11. > What is TF-IDF?

12. > How can you solve a document classification with TF-IDF?

13. > What is the fundamental idea behind language models such as GPT-2
    > and BERT?
